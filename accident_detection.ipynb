{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/navaneethsanil/Accident-Detection/blob/main/accident_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7832ef2f-b3b1-4f7e-8950-0172efdbecc0",
      "metadata": {
        "id": "7832ef2f-b3b1-4f7e-8950-0172efdbecc0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "77682c1b-26d5-48aa-b565-2e5eb64d2743",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77682c1b-26d5-48aa-b565-2e5eb64d2743",
        "outputId": "8d7bb89f-c6b5-4597-b06a-3adeb63c2493"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device is running on cuda\n"
          ]
        }
      ],
      "source": [
        "# Setting up device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Device is running on {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a03452f0-1650-43e2-89cc-224b25ed7a50",
      "metadata": {
        "id": "a03452f0-1650-43e2-89cc-224b25ed7a50"
      },
      "source": [
        "# **Data setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62841d64-d4b2-47d7-b053-e01aa55cb582",
      "metadata": {
        "scrolled": true,
        "id": "62841d64-d4b2-47d7-b053-e01aa55cb582"
      },
      "outputs": [],
      "source": [
        "# Downloading dataset\n",
        "!kaggle datasets download ckay16/accident-detection-from-cctv-footage\n",
        "\n",
        "# Unzip dataset\n",
        "!unzip accident-detection-from-cctv-footage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5528d3c7-4d86-425d-b6a5-2e72d3e626fd",
      "metadata": {
        "id": "5528d3c7-4d86-425d-b6a5-2e72d3e626fd"
      },
      "outputs": [],
      "source": [
        "train_dir = Path(\"data/train\")\n",
        "test_dir = Path(\"data/test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c1db86d1-f366-4202-9f63-0dbb8d5e59e9",
      "metadata": {
        "id": "c1db86d1-f366-4202-9f63-0dbb8d5e59e9"
      },
      "outputs": [],
      "source": [
        "data_transform = transforms.Compose([\n",
        "    transforms.Resize(size=(224, 224)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ToTensor()\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "17308b0a-ff24-4970-8595-c6a4e45ef452",
      "metadata": {
        "id": "17308b0a-ff24-4970-8595-c6a4e45ef452"
      },
      "outputs": [],
      "source": [
        "train_data = datasets.ImageFolder(root=train_dir,\n",
        "                                  transform=data_transform,\n",
        "                                  target_transform=None)\n",
        "\n",
        "test_data = datasets.ImageFolder(root=test_dir,\n",
        "                                 transform=data_transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "37d4e103-66cb-43d9-945f-8d47bf5850a1",
      "metadata": {
        "id": "37d4e103-66cb-43d9-945f-8d47bf5850a1"
      },
      "outputs": [],
      "source": [
        "class_names = train_data.classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "eec98317-3347-48f0-bc44-4bd91567fedd",
      "metadata": {
        "id": "eec98317-3347-48f0-bc44-4bd91567fedd"
      },
      "outputs": [],
      "source": [
        "# Hyper-parameters\n",
        "BATCH_SIZE = 32\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "\n",
        "train_dataloader = DataLoader(dataset=train_data,\n",
        "                             batch_size=BATCH_SIZE,\n",
        "                             num_workers=NUM_WORKERS,\n",
        "                             shuffle=True)\n",
        "\n",
        "test_dataloader = DataLoader(dataset=test_data,\n",
        "                            batch_size=BATCH_SIZE,\n",
        "                            num_workers=NUM_WORKERS,\n",
        "                            shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f624ebf-1719-4f66-9d00-1a5327fccbd4",
      "metadata": {
        "id": "8f624ebf-1719-4f66-9d00-1a5327fccbd4"
      },
      "source": [
        "# **Model building**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6a100e5-198e-4905-b1c6-3c93226d41bf",
      "metadata": {
        "id": "f6a100e5-198e-4905-b1c6-3c93226d41bf"
      },
      "outputs": [],
      "source": [
        "# Transfer Learning\n",
        "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
        "model = torchvision.models.efficientnet_b0(weights=weights).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "742a0886-ad71-4a85-b509-49ebb60d165a",
      "metadata": {
        "id": "742a0886-ad71-4a85-b509-49ebb60d165a"
      },
      "outputs": [],
      "source": [
        "for params in model.features.parameters():\n",
        "  params.require_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "7ae0de02-3266-45a0-b603-cef383b5c733",
      "metadata": {
        "id": "7ae0de02-3266-45a0-b603-cef383b5c733"
      },
      "outputs": [],
      "source": [
        "model.classifier = nn.Sequential(\n",
        "  nn.Dropout(p=0.2, inplace=True),\n",
        "  nn.Linear(in_features=1280, out_features=len(class_names), bias=True)\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "ee9f766b-a176-416d-9df1-49ee76c627ec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee9f766b-a176-416d-9df1-49ee76c627ec",
        "outputId": "ac1a5f60-2e3a-4d26-83c8-6945c9697ee3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# Loading pre-trained model\n",
        "model_path = Path(\"models/accident_detection_model.pth\")\n",
        "\n",
        "model.load_state_dict(torch.load(f=model_path, weights_only=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcae48c6-b5e9-4656-80de-f1a86ecdf971",
      "metadata": {
        "id": "dcae48c6-b5e9-4656-80de-f1a86ecdf971"
      },
      "source": [
        "# **Training and Testing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de9979ed-217e-4d13-b84a-81aff034340c",
      "metadata": {
        "id": "de9979ed-217e-4d13-b84a-81aff034340c"
      },
      "outputs": [],
      "source": [
        "# Setup loss func and optimizer\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params=model.parameters(),\n",
        "                           lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65cb4b4a-fea2-4405-a4e3-78a37535a3af",
      "metadata": {
        "id": "65cb4b4a-fea2-4405-a4e3-78a37535a3af"
      },
      "outputs": [],
      "source": [
        "def train_loop(model: nn.Module,\n",
        "               dataloader: torch.utils.data.DataLoader,\n",
        "               loss_func: nn.Module,\n",
        "               optimizer: torch.optim.Optimizer):\n",
        "    model.train()\n",
        "\n",
        "    train_loss, train_acc = 0, 0\n",
        "\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "      X, y = X.to(device), y.to(device)\n",
        "\n",
        "      y_pred = model(X)\n",
        "      loss = loss_func(y_pred, y)\n",
        "      train_loss += loss.item()\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
        "      train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
        "\n",
        "\n",
        "    train_loss = train_loss / len(dataloader)\n",
        "    train_acc = train_acc / len(dataloader)\n",
        "\n",
        "    return train_loss, train_acc\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def test_loop(model: nn.Module,\n",
        "              dataloader: torch.utils.data.DataLoader,\n",
        "              loss_func: nn.Module):\n",
        "    model.eval()\n",
        "\n",
        "    test_loss, test_acc = 0, 0\n",
        "\n",
        "    with torch.inference_mode():\n",
        "      for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        test_pred = model(X)\n",
        "        loss = loss_func(test_pred, y)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        test_pred_labels = test_pred.argmax(dim=1)\n",
        "        test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
        "\n",
        "    test_loss = test_loss / len(dataloader)\n",
        "    test_acc = test_acc / len(dataloader)\n",
        "\n",
        "    return test_loss, test_acc\n",
        "\n",
        "\n",
        "\n",
        "def train(epochs: int,\n",
        "          model: nn.Module,\n",
        "          train_dataloader: torch.utils.data.DataLoader,\n",
        "          test_dataloader: torch.utils.data.DataLoader,\n",
        "          loss_func: nn.Module,\n",
        "          optimizer: torch.optim.Optimizer):\n",
        "\n",
        "    results = {\n",
        "        \"train_loss\": [],\n",
        "        \"train_acc\": [],\n",
        "        \"test_loss\": [],\n",
        "        \"test_acc\": []\n",
        "    }\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "      train_loss, train_acc = train_loop(model=model,\n",
        "                                        dataloader=train_dataloader,\n",
        "                                        loss_func=loss_func,\n",
        "                                        optimizer=optimizer)\n",
        "\n",
        "      test_loss, test_acc = test_loop(model=model,\n",
        "                                      dataloader=test_dataloader,\n",
        "                                      loss_func=loss_func)\n",
        "\n",
        "      print(\n",
        "          f\"Epoch: {epoch+1} | \"\n",
        "          f\"train_loss: {train_loss:.4f} | \"\n",
        "          f\"train_acc: {train_acc:.4f} | \"\n",
        "          f\"test_loss: {test_loss:.4f} | \"\n",
        "          f\"test_acc: {test_acc:.4f}\"\n",
        "      )\n",
        "\n",
        "      results[\"train_loss\"].append(train_loss.item() if isinstance(train_loss, torch.Tensor) else train_loss)\n",
        "      results[\"train_acc\"].append(train_acc.item() if isinstance(train_acc, torch.Tensor) else train_acc)\n",
        "      results[\"test_loss\"].append(test_loss.item() if isinstance(test_loss, torch.Tensor) else test_loss)\n",
        "      results[\"test_acc\"].append(test_acc.item() if isinstance(test_acc, torch.Tensor) else test_acc)\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bce85c3-8859-4fed-995c-1f9102172d57",
      "metadata": {
        "scrolled": true,
        "id": "0bce85c3-8859-4fed-995c-1f9102172d57"
      },
      "outputs": [],
      "source": [
        "torch.cuda.manual_seed(42)\n",
        "model_results = train(epochs=5,\n",
        "                      model=model,\n",
        "                      train_dataloader=train_dataloader,\n",
        "                      test_dataloader=test_dataloader,\n",
        "                      loss_func=loss_func,\n",
        "                      optimizer=optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "861f0ff6-1153-4126-8d1d-2ad384f6accf",
      "metadata": {
        "id": "861f0ff6-1153-4126-8d1d-2ad384f6accf"
      },
      "source": [
        "# **Saving model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "064a3691-0e8b-4df3-ba03-8a4bab6951e7",
      "metadata": {
        "id": "064a3691-0e8b-4df3-ba03-8a4bab6951e7"
      },
      "outputs": [],
      "source": [
        "MODEL_PATH = Path(\"models\")\n",
        "MODEL_NAME = \"accident_detection_model.pth\"\n",
        "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
        "torch.save(obj=model.state_dict(),\n",
        "           f=MODEL_SAVE_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b4c6578-54de-44e3-b0ce-798f054426c5",
      "metadata": {
        "id": "6b4c6578-54de-44e3-b0ce-798f054426c5"
      },
      "source": [
        "# **Gradio Demo**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "644812e3-897e-44a6-b2bf-11ca361806dc",
      "metadata": {
        "id": "644812e3-897e-44a6-b2bf-11ca361806dc"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from PIL import Image\n",
        "import gradio as gr\n",
        "import geocoder\n",
        "from geopy.geocoders import Nominatim\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "434a8e9b-6ee6-4587-9954-4cd934b780aa",
      "metadata": {
        "id": "434a8e9b-6ee6-4587-9954-4cd934b780aa"
      },
      "outputs": [],
      "source": [
        "def predict(video_path: \"str\"):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    if not cap.isOpened():\n",
        "      print(\"Error: Could not open video file.\")\n",
        "      exit()\n",
        "\n",
        "    while True:\n",
        "      ret, frame = cap.read()\n",
        "\n",
        "      if not ret:\n",
        "        break\n",
        "\n",
        "      frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "      image_transform = transforms.Compose([\n",
        "        transforms.Resize(size=(224, 224)),\n",
        "        transforms.ToTensor()\n",
        "      ])\n",
        "\n",
        "      frame = image_transform(frame).unsqueeze(dim=0).to(device)\n",
        "      model.eval()\n",
        "      with torch.inference_mode():\n",
        "        y_pred = model(frame).to(device)\n",
        "        y_pred_label = torch.argmax(y_pred, dim=1)\n",
        "\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "    if class_names[y_pred_label] == \"Accident\":\n",
        "      # Get current location using IP (Internet-based location)\n",
        "      g = geocoder.ip('117.207.239.24')\n",
        "\n",
        "      # Extract latitude and longitude\n",
        "      latitude, longitude = g.latlng\n",
        "\n",
        "      # Create a geolocator object\n",
        "      geolocator = Nominatim(user_agent=\"accident_detection\")\n",
        "\n",
        "      # Reverse geocoding: Get address from latitude and longitude\n",
        "      location = geolocator.reverse((latitude, longitude), language='en')\n",
        "\n",
        "      return f\"Accident detected at {location.address}\"\n",
        "    else:\n",
        "      return \"Non accident\"\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "689d4206-220d-4a43-b681-982f1463da2d",
      "metadata": {
        "scrolled": true,
        "id": "689d4206-220d-4a43-b681-982f1463da2d",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "outputId": "db99eca5-205b-4e87-b701-4d1ef427c7df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://36683bc7def144e986.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://36683bc7def144e986.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "# To view the demonstration, you may input a video by running the cell below.\n",
        "demo = gr.Interface(predict,\n",
        "                   inputs=gr.Video(),\n",
        "                   outputs=gr.Label())\n",
        "\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mr8mMGLyHPFB"
      },
      "id": "Mr8mMGLyHPFB",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "accident_detection_ipykernel",
      "language": "python",
      "name": "accident_detection_ipykernel"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}