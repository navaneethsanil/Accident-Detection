{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7832ef2f-b3b1-4f7e-8950-0172efdbecc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77682c1b-26d5-48aa-b565-2e5eb64d2743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is running on cuda\n"
     ]
    }
   ],
   "source": [
    "# Setting up device agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device is running on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03452f0-1650-43e2-89cc-224b25ed7a50",
   "metadata": {},
   "source": [
    "# **Data setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62841d64-d4b2-47d7-b053-e01aa55cb582",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Downloading dataset\n",
    "!kaggle datasets download ckay16/accident-detection-from-cctv-footage\n",
    "\n",
    "# Unzip dataset\n",
    "!unzip accident-detection-from-cctv-footage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5528d3c7-4d86-425d-b6a5-2e72d3e626fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = Path(\"data/train\")\n",
    "test_dir = Path(\"data/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1db86d1-f366-4202-9f63-0dbb8d5e59e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize(size=(224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17308b0a-ff24-4970-8595-c6a4e45ef452",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.ImageFolder(root=train_dir,\n",
    "                                  transform=data_transform,\n",
    "                                  target_transform=None)\n",
    "\n",
    "test_data = datasets.ImageFolder(root=test_dir,\n",
    "                                 transform=data_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37d4e103-66cb-43d9-945f-8d47bf5850a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_data.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eec98317-3347-48f0-bc44-4bd91567fedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_data,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             num_workers=NUM_WORKERS,\n",
    "                             shuffle=True)\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_data,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            num_workers=NUM_WORKERS,\n",
    "                            shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f624ebf-1719-4f66-9d00-1a5327fccbd4",
   "metadata": {},
   "source": [
    "# **Model building**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6a100e5-198e-4905-b1c6-3c93226d41bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer Learning\n",
    "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
    "model = torchvision.models.efficientnet_b0(weights=weights).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "742a0886-ad71-4a85-b509-49ebb60d165a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for params in model.features.parameters():\n",
    "    params.require_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ae0de02-3266-45a0-b603-cef383b5c733",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier = nn.Sequential(\n",
    "    nn.Dropout(p=0.2, inplace=True),\n",
    "    nn.Linear(in_features=1280, out_features=len(class_names), bias=True)\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee9f766b-a176-416d-9df1-49ee76c627ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading pre-trained model\n",
    "model_path = Path(\"models/accident_detection_model.pth\")\n",
    "\n",
    "model.load_state_dict(torch.load(f=model_path, weights_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4c6578-54de-44e3-b0ce-798f054426c5",
   "metadata": {},
   "source": [
    "# **Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "644812e3-897e-44a6-b2bf-11ca361806dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "434a8e9b-6ee6-4587-9954-4cd934b780aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(video_path: \"str\"):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video file.\")\n",
    "        exit()\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "    \n",
    "        if not ret:\n",
    "                print(\"End of video.\")\n",
    "                break\n",
    "    \n",
    "        frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        image_transform = transforms.Compose([\n",
    "            transforms.Resize(size=(224, 224)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "    \n",
    "        frame = image_transform(frame).unsqueeze(dim=0).to(device)\n",
    "        model.eval()\n",
    "        with torch.inference_mode():\n",
    "            y_pred = model(frame).to(device)\n",
    "            y_pred_label = torch.argmax(y_pred, dim=1)\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    return class_names[y_pred_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "689d4206-220d-4a43-b681-982f1463da2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of video.\n",
      "End of video.\n"
     ]
    }
   ],
   "source": [
    "# To view the demonstration, you may input a video by running the cell below.\n",
    "demo = gr.Interface(predict,\n",
    "                   inputs=gr.Video(),\n",
    "                   outputs=gr.Label())\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcae48c6-b5e9-4656-80de-f1a86ecdf971",
   "metadata": {},
   "source": [
    "# **Training and Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "de9979ed-217e-4d13-b84a-81aff034340c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup loss func and optimizer\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(params=model.parameters(),\n",
    "                           lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "65cb4b4a-fea2-4405-a4e3-78a37535a3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: nn.Module,\n",
    "              dataloader: torch.utils.data.DataLoader,\n",
    "              loss_func: nn.Module,\n",
    "              optimizer: torch.optim.Optimizer):\n",
    "    model.train()\n",
    "\n",
    "    train_loss, train_acc = 0, 0\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        y_pred = model(X)\n",
    "        loss = loss_func(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
    "\n",
    "\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "    \n",
    "    return train_loss, train_acc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test_step(model: nn.Module,\n",
    "             dataloader: torch.utils.data.DataLoader,\n",
    "             loss_func: nn.Module):\n",
    "    model.eval()\n",
    "\n",
    "    test_loss, test_acc = 0, 0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            test_pred = model(X)\n",
    "            loss = loss_func(test_pred, y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            test_pred_labels = test_pred.argmax(dim=1)\n",
    "            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
    "\n",
    "    test_loss = test_loss / len(dataloader)\n",
    "    test_acc = test_acc / len(dataloader)\n",
    "    \n",
    "    return test_loss, test_acc\n",
    "\n",
    "\n",
    "\n",
    "def train(epochs: int,\n",
    "         model: nn.Module,\n",
    "         train_dataloader: torch.utils.data.DataLoader,\n",
    "         test_dataloader: torch.utils.data.DataLoader,\n",
    "         loss_func: nn.Module,\n",
    "         optimizer: torch.optim.Optimizer):\n",
    "    \n",
    "    results = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"test_acc\": []\n",
    "    }\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                          dataloader=train_dataloader,\n",
    "                                          loss_func=loss_func,\n",
    "                                          optimizer=optimizer)\n",
    "\n",
    "        test_loss, test_acc = test_step(model=model,\n",
    "                                       dataloader=test_dataloader,\n",
    "                                       loss_func=loss_func)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch: {epoch+1} | \"\n",
    "            f\"train_loss: {train_loss:.4f} | \"\n",
    "            f\"train_acc: {train_acc:.4f} | \"\n",
    "            f\"test_loss: {test_loss:.4f} | \"\n",
    "            f\"test_acc: {test_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        results[\"train_loss\"].append(train_loss.item() if isinstance(train_loss, torch.Tensor) else train_loss)\n",
    "        results[\"train_acc\"].append(train_acc.item() if isinstance(train_acc, torch.Tensor) else train_acc)\n",
    "        results[\"test_loss\"].append(test_loss.item() if isinstance(test_loss, torch.Tensor) else test_loss)\n",
    "        results[\"test_acc\"].append(test_acc.item() if isinstance(test_acc, torch.Tensor) else test_acc)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bce85c3-8859-4fed-995c-1f9102172d57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.manual_seed(42)\n",
    "model_results = train(epochs=5,\n",
    "                     model=model,\n",
    "                     train_dataloader=train_dataloader,\n",
    "                     test_dataloader=test_dataloader,\n",
    "                     loss_func=loss_func,\n",
    "                     optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861f0ff6-1153-4126-8d1d-2ad384f6accf",
   "metadata": {},
   "source": [
    "# **Saving model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "064a3691-0e8b-4df3-ba03-8a4bab6951e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = Path(\"models\")\n",
    "MODEL_NAME = \"accident_detection_model.pth\"\n",
    "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
    "torch.save(obj=model.state_dict(), # only saving the state_dict() only saves the learned parameters\n",
    "           f=MODEL_SAVE_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
